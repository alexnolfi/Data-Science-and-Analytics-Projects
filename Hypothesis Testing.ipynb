{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oGjoWSWqQ77"
   },
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oi4JzShqQ78"
   },
   "source": [
    "This assignmemt is based on content discussed in module 3 and test basic concepts of probability theory and normalization in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjI1J5nXqQ79"
   },
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9t0LyaqqQ7-"
   },
   "source": [
    "-   Work on problems of different distributions eg., binomial, gaussian \n",
    "-   Calculate z score \n",
    "-\t Make statistical inferences on given data\n",
    "-\t Construct a null and an alternate hypothesis\n",
    "-\t Find the p-value for a given hypothesis and T test statistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lXvCMN_qQ7_"
   },
   "source": [
    "**Question 1**\n",
    "\n",
    "The Capital Asset Pricing Model (CAPM) is a financial model that assumes returns on a portfolio are normally distributed.  Suppose a portfolio has an average annual return of 14.7% (i.e., an average gain on 14.7%) with a standard deviation of 33%.  A return of 0% means the value of the portfolio doesn't change, a negative return means that the portfolio loses money, and a positive return means that the portfolio gains money. Determine the following:\n",
    "\n",
    "1. What percentage of years does this portfolio lose money, (i.e. have a return less than 0%)?\n",
    "2. What is the cutoff for the highest 15% of annual returns with this portfolio?\n",
    "\n",
    "See CAPM here https://en.wikipedia.org/wiki/Capital_asset_pricing_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_l53E9BAqQ8A"
   },
   "source": [
    "**Question 2**\n",
    "\n",
    "Past experience indicates that because of low morale, a company loses 20 hours a year per employee due to lateness and abstenteeism.  Assume that the standard deviation of the population is 6 and normally distributed.\n",
    "\n",
    "The HR department implemented a new rewards system to increase employee morale, and after a few months it collected a random sample of 20 employees and the annualized absenteeism was 14.\n",
    "\n",
    "1. Could you confirm that the new rewards system was effective with a 90% confidence?\n",
    "2. An HR subject matter expert would be very happy if the program could reduce absenteeism by 20% (i.e. to 16 hours).  Given the current sampling parameters, what is the probability that the new rewards system reduced absenteeism to 16 hours and you miss it?\n",
    "3. Repeat part 1) and 2) with an α = 95% CI.\n",
    "4. Based on the answers in 3), is the sampling method good enough to identify a reduction from 20 to 16 hours if I use a confidence of 95%?\n",
    "5. What should the sample size be if you want β to be 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOjrz6ycqQ8A"
   },
   "source": [
    "**Question 3**\n",
    "\n",
    "Chi-Square Goodness of fit\n",
    "\n",
    "Please access and review **section 6.3.5** in the OpenIntro Statistics textbook:\n",
    "\n",
    "Diez, D., Barr, C. & Çetinkaya-Rundel, M. (2017). OpenIntro Statistics (3rd Ed.). https://www.openintro.org/stat/textbook.php?stat_book=os\n",
    "\n",
    "Given the information in section 6.3.5, write python code for the following:\n",
    "\n",
    " - Calculate the expected values based on the geometric distribution with a probability of 53.2%\n",
    " - Compare the expected vs. the observed values from the textbook using the Chi-Square distribution\n",
    " - Reach a conclusion\n",
    " - Explain what is the business impact of your conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.44545454545454544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3279956507031998"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "#Part 1: What percentage of years does this portfolio lose money?\n",
    "\n",
    "#The probability that the portfolio loses money in a given year can be calculated using z-scores, so first, find the z-score\n",
    "#for 0 with a population mean of 14.7 and a standard deviation of 33\n",
    "mean=14.7; stdev=33\n",
    "z_score=(0-mean)/stdev\n",
    "print(z_score)\n",
    "\n",
    "#We see that the z-score is negative, which makes sense intuitively since 0 is less than 14. Now, it's possible to convert \n",
    "#this z-score into a probability using scipy (or standard normal tables) since we've normalized our data.\n",
    "import scipy.stats as st\n",
    "st.norm.cdf(z_score) \n",
    "\n",
    "#The value of approxiamtely 0.328 suggests that 32.8% of the area under the curve lies to the left of x=0\n",
    "#Thus, this portfolio loses money about 33% of the time, or 1 out of every 3 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0364333894937898\n",
      "48.90230185329506 This is the cutoff\n"
     ]
    }
   ],
   "source": [
    "#Part 2: What is the cutoff for the highest 15% of annual returns with this portfolio?\n",
    "\n",
    "#For this part, we need to identify the annual return that corresponds to top 15% of returns, so first we need to find the\n",
    "#z-score that covers the bottom 85% of the data\n",
    "z_score=st.norm.ppf(0.85)\n",
    "print(z_score)\n",
    "\n",
    "#Using the equation z-score=(x-mean)/stdev, we find: 1.036=(x-14.7)/33. Thus:\n",
    "cutoff=z_score*stdev + mean\n",
    "print(cutoff, \"This is the cutoff\")\n",
    "\n",
    "#The cutoff for the highest 15% of annual returns in this portfolio is approximately 48.9% (i.e. in 15% of years, we should\n",
    "#expect higher annual returns than this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.872108215522035e-06\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "import math\n",
    "\n",
    "#Part 1\n",
    "\n",
    "#Although we know the population standard deviation, we need to use a t-test (one sample) in this case, as sample size is \n",
    "#less than 30\n",
    "\n",
    "#The null hypothesis is that the new rewards system had no effect on employee lateness/absenteeism, while the alternative\n",
    "#hypothesis is that the new rewards system REDUCED lateness and absenteeism (thus, we use a one-tailed test)\n",
    "pop_mean=20\n",
    "sample_mean=14\n",
    "pop_stdev=6\n",
    "n=20\n",
    "\n",
    "#To determine the area under the curve to the left of 14 (our sample mean), we can use st.norm.cdf \n",
    "p_val=st.norm.cdf(sample_mean, loc=pop_mean, scale=pop_stdev/math.sqrt(pop_mean))\n",
    "print(p_val)\n",
    "\n",
    "#At a confidence level of 90%, we can safely reject the null hypothesis, as p-val is close to 0. Thus, we can say the new system\n",
    "#has been effective at reducing lateness/absenteeism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044577464303378944\n"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "\n",
    "#Assuming a significance level of 0.1, we can identify all values for which we reject the null using z=(x-mean)/(stdev/sqrt(n))\n",
    "#The z-score that corresponds to p=0.1 can be found as:\n",
    "z_score=st.norm.ppf(0.1)\n",
    "\n",
    "#Then, x can be found as:\n",
    "x=z_score*(6/math.sqrt(20))+20 #Any values of x greater than 18.28 will result in failing to reject the null\n",
    "\n",
    "#If we assume the population mean is actually 16, then we can find the probability that x is greater than 18.18 as:\n",
    "z_score_2 = (x-16)/(6/math.sqrt(20))\n",
    "\n",
    "#Convert this z-score to a probability\n",
    "prob=st.norm.cdf(z_score_2)\n",
    "\n",
    "#The probability we obtain a value lower than 18.28 is approximately 0.995, so the probability we obtain a value greater than\n",
    "#18.28 is:\n",
    "final_prob=1-prob\n",
    "print(final_prob) #This is the probablity we accidentally \"miss\" the 4 (20 to 16) hr improvement of the reward system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.872108215522035e-06\n"
     ]
    }
   ],
   "source": [
    "#Part 3A\n",
    "\n",
    "#Reusing our work from part 1, we can determine whether our finding is significant at a 95% confidence level\n",
    "\n",
    "#The null hypothesis is that the new rewards system had no effect on employee lateness/absenteeism, while the alternative\n",
    "#hypothesis is that the new rewards system REDUCED lateness and absenteeism (thus, we use a one-tailed test)\n",
    "pop_mean=20\n",
    "sample_mean=14\n",
    "pop_stdev=6\n",
    "n=20\n",
    "\n",
    "#To determine the area under the curve to the left of 14 (our sample mean), we can use st.norm.cdf \n",
    "p_val=st.norm.cdf(sample_mean, loc=pop_mean, scale=pop_stdev/math.sqrt(pop_mean))\n",
    "print(p_val)\n",
    "\n",
    "#At a confidence level of 95%, we can safely reject the null hypothesis, as p-val is close to 0. Thus, we can say the new system\n",
    "#has been effective at reducing lateness/absenteeism, even at a 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09068146248885567\n"
     ]
    }
   ],
   "source": [
    "#Part 3B\n",
    "\n",
    "#Assuming a significance level of 0.05, we can identify all values for which we reject the null using z=(x-mean)/(stdev/sqrt(n))\n",
    "#The z-score that corresponds to p=0.05 can be found as:\n",
    "z_score=st.norm.ppf(0.05)\n",
    "\n",
    "#Then, x can be found as:\n",
    "x=z_score*(6/math.sqrt(20))+20 #Any values of x greater than 17.79 will result in failing to reject the null\n",
    "\n",
    "#If we assume the pop mean is actually 16, then we can find the probability that x is greater than 17.79 as:\n",
    "z_score_2 = (x-16)/(6/math.sqrt(20))\n",
    "\n",
    "#Convert this z-score to a probability\n",
    "prob=st.norm.cdf(z_score_2)\n",
    "\n",
    "#The probability we obtain a value lower than 17.79 is approximately 0.909, so the probability we obtain a value greater than\n",
    "#17.79 is:\n",
    "final_prob=1-prob\n",
    "print(final_prob) #This is the probablity we accidentally \"miss\" the 4 (20 to 16) hr improvement of the reward system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014345563960383074\n"
     ]
    }
   ],
   "source": [
    "#Part 4\n",
    "\n",
    "#Here, we run a similar test as part 3A, but we use a sample mean of 16 as opposed to 14\n",
    "\n",
    "#The null hypothesis is that the new rewards system had no effect on employee lateness/absenteeism, while the alternative\n",
    "#hypothesis is that the new rewards system REDUCED lateness and absenteeism (thus, we use a one-tailed test)\n",
    "pop_mean=20\n",
    "sample_mean=16\n",
    "pop_stdev=6\n",
    "n=20\n",
    "\n",
    "#To determine the area under the curve to the left of 14 (our sample mean), we can use st.norm.cdf \n",
    "p_val=st.norm.cdf(sample_mean, loc=pop_mean, scale=pop_stdev/math.sqrt(pop_mean))\n",
    "print(p_val)\n",
    "\n",
    "#At a confidence level of 95%, we can safely reject the null hypothesis, as p-val=0.001. Thus, we can say the new system\n",
    "#has been effective at reducing lateness/absenteeism, even at a 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.403826347492267\n"
     ]
    }
   ],
   "source": [
    "#Part 5\n",
    "\n",
    "#As in part 3B, we assume:\n",
    "pop_mean=20\n",
    "sample_mean=16\n",
    "pop_stdev=6\n",
    "n=20\n",
    "\n",
    "#Assuming a significance level of 0.05, we can identify all values for which we reject the null using z=(x-mean)/(stdev/sqrt(n))\n",
    "#The z-score that corresponds to p=0.05 can be found as:\n",
    "z_score=st.norm.ppf(0.05)\n",
    "\n",
    "#Then, x can be found as:\n",
    "x=z_score*(6/math.sqrt(20))+20 #Any values of x greater than 17.79 will result in failing to reject the null\n",
    "\n",
    "#Here, we can set z_score_2 to a value that would result in a beta of 0.05 (-1.645) and solve for n\n",
    "#-1.645 = (x-16)/(6/math.sqrt(n)), which is equivalent to:\n",
    "n=(-9.87/1.79)**2\n",
    "print(n)\n",
    "\n",
    "#Thus, we would need a sample size of approximately 30 to ensure a beta of 0.05 with a confidence level of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Observed    Expected\n",
      "Days                      \n",
      "1          717  724.584000\n",
      "2          369  339.105312\n",
      "3          155  158.701286\n",
      "4           69   74.272202\n",
      "5           28   34.759390\n",
      "6           14   16.267395\n",
      "7+          10   14.160057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=6.028048145581277, pvalue=0.4200554577210595)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 3\n",
    "import pandas as pd\n",
    "\n",
    "#Part 1 (geometric with P(Success)=0.532)\n",
    "#For convenience, I'll reproduce the observed counts from the textbook here\n",
    "observed = [717, 369, 155, 69, 28, 14, 10]\n",
    "\n",
    "#Using the pmf of the geometric distribution [(1-p)**(k-1)*p], we can calculate the expected counts with P(Success)=0.532 \n",
    "#and total of 1362 observations\n",
    "expected_1 = ((1-0.532)**(1-1))*0.532*1362\n",
    "expected_2 = ((1-0.532)**(2-1))*0.532*1362\n",
    "expected_3 = ((1-0.532)**(3-1))*0.532*1362\n",
    "expected_4 = ((1-0.532)**(4-1))*0.532*1362\n",
    "expected_5 = ((1-0.532)**(5-1))*0.532*1362\n",
    "expected_6 = ((1-0.532)**(6-1))*0.532*1362\n",
    "expected_7_plus = (((1-0.532)**(7-1))*0.532*1362 + ((1-0.532)**(8-1))*0.532*1362 + ((1-0.532)**(9-1))*0.532*1362 +\n",
    "((1-0.532)**(10-1))*0.532*1362 + ((1-0.532)**(11-1))*0.532*1362 + ((1-0.532)**(12-1))*0.532*1362)\n",
    "    \n",
    "#For the final bin (expected_7_plus), we could keep adding terms indefinitely, as this technically goes to infinity, \n",
    "#but stopping at 12 gives a sufficient approximation\n",
    "\n",
    "#Create list with expected values\n",
    "expected = [(expected_1), (expected_2), (expected_3), (expected_4), \n",
    "            (expected_5), (expected_6), (expected_7_plus)]\n",
    "\n",
    "#Create dataframe and rename indices\n",
    "df_3 = pd.DataFrame({\"Observed\": observed, \"Expected\": expected})\n",
    "df_3=df_3.rename(index={0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\", 5: \"6\", 6: \"7+\"})\n",
    "df_3.index.name = \"Days\"\n",
    "print(df_3)\n",
    "df_3.sum()\n",
    "\n",
    "#Part 2 - Use Chi-Square distribution for hypothesis test\n",
    "\n",
    "#The null hypothesis is that all trading days' price changes are independnet from one another, while the alternative hypothesis\n",
    "#is that trading days' price changes are not independent of one another\n",
    "import scipy.stats as st\n",
    "st.chisquare(f_obs=observed, f_exp=expected)\n",
    "\n",
    "#Parts 3 and 4 - Conclusion/Business Implication\n",
    "#At any reasonable significance level (0.1, 0.05, etc.), we fail to reject the null hypothesis that the stock market being\n",
    "#up or down on a given day is independent from all other days, as the p-value is approximately 0.42\n",
    "\n",
    "#In other words, we cannot prove that the notion of trading days being independent from one another is incorrect\n",
    "\n",
    "#From a business standpoint, this implies that it is NOT useful to use yesterday's change in price (e.g. up/down)\n",
    "#as a predictor of today's change in price"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3v.3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
